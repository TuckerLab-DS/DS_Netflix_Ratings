---
title: "MovieLens Capstone"
date: "14/03/2021"
author: "CT"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
#Set knitr gloval options. Turn off echo,warnings and message to prevent these items showing up in final report. 
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE)
```

```{r start_time}
#save the time this code started running. This is used at the end to calculate total run time for this model.
start_time <- Sys.time()
```

## Introduction
<!-- Rubric: Section that describes the dataset and summarizes the goal of the project and key steps that were performed -->
**This project will create a movie recommendation system using the MovieLens dataset, and a machine learning algorithm.**

### About the dataset
The dataset used is the 10M version of the MovieLens dataset. The dataset contains 10000054 ratings and 95580 tags applied to 10681 movies by 71567 users of the online movie recommender service MovieLens. Users were selected at random for inclusion. All users selected had rated at least 20 movies. Each user is represented by an id, and no other personal identifiable information is provided. More information on the dataset can be found here: https://grouplens.org/datasets/movielens/10m/

### Goal
The machine learning algorithm will use the inputs in one subset (Training Set) to predict movie ratings in another set (Validation Set) A number of algorithms will be trialed to determine the optimum mixture of features to obtain a movie rating prediction with the lowest Root Mean Square Estimate (RMSE). **The goal RSME is <0.86490**.

*Note: The final validation dataset will only be used at the end of the project to assess the final model. It will not be used to test the RMSE of multiple models during model development.*

### Key Steps
Key Steps in the project:

1. Investigate the structure of the training data set, generate plots to visualise data where required.
2. Pre-process the data
    + Clean/Wrangle data
    + Standardise or transforming predictors
    + Remove predictors that are not useful, are highly correlated with others, have very few non-unique values, or have close to zero variation.
3. Split the Training (edx) data into Training and Test set in order to evaluate algorithms
4. Build a base algorithm, and include additional features/biases and/or regularisation to optimise
5. Summarise data, choosing optimal algorithm based on training/test data 
6. Apply algorithm to validation dataset

## Method/Analysis
<!-- Section that explains the process and techniques used, including data cleaning, data exploration and visualization, insights gained, and your modeling approach -->

### Library ###

```{r library_packages, echo=FALSE, warning=FALSE, message=FALSE}
#####
#Code to install all required librarys
#####

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(dplyr)
library(data.table)
library(lubridate)
library(knitr)

```

The following packages/versions have been used, this includes dependencies:
<!-- comment=NA is used to remove the ## from the printed data-->
```{r show_package, echo=FALSE, comment=NA}

########
#Code generates a list of used packages/versions to show in report
########

    # Get all non-base packages in sessions
    packages <- names(sessionInfo()$otherPkgs)

    # Sort list alphabetically
    packages <- sort(packages)
    
    #get associated version for each package, into a list
    packages <- sapply(packages, packageVersion, simplify=FALSE)
    
    #turn into a character
    packages <- sapply(packages, as.character)
    
    #turn into a dataframe to make it nice to print to screen
    packages <- data.frame(packages)

    #Move rowname into first column of dataframe, assign header, and then move version into second  column
    packages <- (data.frame(package=rownames(packages), version=packages[1], row.names=NULL))
    
    #Print to screen
    print(packages, row.names = FALSE)
```
 
### Initial Data Load
Data downloaded from the 10m Dataset has two files: Ratings and Movies. The initial data wrangle extracts these files, and stores them into a data-frame. This data-frame is then split into two: 'edx' (Model Data) and 'validation' (Final Validation Data). The validation set is set at 10% of the MovieLens data. 
The Model Data is further split into two: Train Data (used for training the algorithms) and Test Data (for verifying the models), with the Test data set at 10% of the Train data.


```{r initial_data_load, echo=FALSE, warning=FALSE, message=FALSE, results = FALSE}
########
#Code in this section provided as part of project, and is used to download the initial movelens dataset, and store in a two dataframe's: 'edx' (Train Data) and 'validation' (Final validation data). The
########

#Increase timeout to 360s because my internet is really slow
options(timeout=360)

#Create tempfile and download movelens data to tempfile.
dl <- tempfile()

#updated to include error handling when download fails. 
last.message <- NULL
tryCatch( { download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl); 
            print("Download Complete") }
          , message = function(m) { print("Error") })

#Initial wrange of unzip (decompress) data, and store in ratings/movies variables, then merge together to create the movelens dataframe
ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)

#add in column names
colnames(movies) <- c("movieId", "title", "genres")

#slightly different code runs dependent on R Version. Creates movies data-frame
if (R.version$major>=4) {
  # if using R 4.0 or later:
  movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                             title = as.character(title),
                                             genres = as.character(genres))
} else {
  #if using R 3.6 or earlier:
  movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                            title = as.character(title),
                                            genres = as.character(genres))
}

#combines ratings and movies dataframe using movieID
movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data. Set the random seed. 
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>%
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

#remove and clean up objects that arent going to be used anymore
rm(dl, ratings, movies, test_index, temp, movielens, removed)


# Create test_set data using 10% of edx data. This is the data that will be used to verify models, because we cant use the validation data for this as set by project scope
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`

test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.1, list = FALSE)

train_set <- edx[-test_index,]
temp <- edx[test_index,]

# Make sure userId and movieId in test set are also in train set
test_set <- temp %>%
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")

# Add rows removed from test set back into train set
removed <- anti_join(temp, test_set)
train_set <- rbind(train_set, removed)

#clean up variables
rm(test_index, temp, removed)

```


### Data Structure ###

The structure of the data is shown below. Its worth noting that the year the movie was released is not a separate data field, and is a part of the Movie Title.  As an outcome of this investigation the movie release year has been split into its own column so further analysis can be performed to determine if this should be a predictor.
The rated year (timestamp) needs to be converted into a Year format (YYYY) to complete further analysis.

In addition, the genres are split by a pipe delimiter (|) - at this stage no manipulation of this data-field will be performed.

```{r, echo=FALSE}

#####
#Mutate data-set with required fields.
#####

#adds columns into all datasets (train, test and validate) for year rated (converted from timestampe fomat) and year released which comes from the last few numbers in the title.
train_set <- train_set %>% 
  mutate(year_released = str_sub(title, -5, -2)) %>%
  mutate(year_rate = year(as_datetime(timestamp)))

test_set <- test_set %>% 
  mutate(year_released = str_sub(title, -5, -2)) %>%
  mutate(year_rate = year(as_datetime(timestamp)))

validation_set <- validation %>% 
  mutate(year_released = str_sub(title, -5, -2)) %>%
  mutate(year_rate = year(as_datetime(timestamp)))

```


```{r data_structure, comment=""}
# Remove the attr from the str print, makes the output look a bit nicer. Print the structure of the data-set. This is used for data exploration purposes
attr(edx, ".internal.selfref") <- NULL
str(edx)
```

The data-set contains the following number of rows and columns:

```{r data_rows}

#number of rows/columns in data set
kable(dim(edx), "simple")
```

There are the following number of unique rows for the predictors:
```{r unique_rows}

#print number of unique rows for various predictors in data-set. 
uniqueRows <- train_set %>% summarize(n_users = n_distinct(userId),
            n_movies = n_distinct(movieId), n_genre = n_distinct(genres), n_year_rel = n_distinct(year_released), n_year_rate = n_distinct(year_rate) )

kable(uniqueRows, "simple")

```

The following data shows the distribution of users and movies. We can see some movies are rated a lot more than others, and also some users rate more movies than other. This indicates that regularisation might be required when using these as predictors. Regularisation can optimise the model by penalising large estimates that are formed using small sample sizes. 

```{r user_movie_distribution}

#plot a chart showing the distribution of number of ratings per user
train_set %>% group_by(userId) %>% summarise(n = n()) %>% group_by(n) %>% summarise(n_c = n()) %>% qplot(x=n, data=., xlab="n", ylab="Count",  main="User", color = I("black"))

#plot a chart showing the distribution of number of ratings per movie
train_set %>% group_by(movieId) %>% summarise(n = n()) %>% group_by(n) %>% summarise(n_c = n()) %>%
qplot(x=n, data=., xlab="n", ylab="Count", main="Movie", color = I("black"))


```

## Results

<!-- section that presents the modeling results and discusses the model performance -->

### First Model 
In the first model we predict the same rating for all movies regardless of user. The predicted rating is the average rating across all rows. The chart below shows the distribution of ratings, the average (blue dotted line).

```{r target_rmse}
#create a tibble with a row containing our target value.
rmse_results <- tibble(method = "Target", RMSE = "<0.86490")
```

```{r first_model}

#####
#Simple first model: we predict the same rating for all movies regardless of user
#####

#calculate mean of the ratings.
mu <- mean(train_set$rating)

#calculate RSME of the ratings using the MU.
rmse <- RMSE(test_set$rating, mu)

#plot the distribution of ratings, with a line showing the average.
qplot(rating, data = test_set, bins = 10, color = I("black")) + geom_vline(xintercept = mu, linetype="dotted", color = "blue", size=1.5)

# add a row to our results tibble with the calculated RSME for this method
rmse_results <- rmse_results %>% add_row(tibble_row(method = "Average", RMSE = as.character(round(rmse,5))))

#print the Target value, and the RSME for this method.
kable(rmse_results[c(which(rmse_results$method=="Target"), which(rmse_results$method=="Average")),], "simple")

```

This RSME is in this first model is above the target of 0.86490, and so further optimisation is required. 

### First Bias (Movie ID)
Next we add in our first bias: the Movie ID. This bias shifts the estimated user rating based on the average rating of each movie (based on all users). From the plot we can see how this distributes ratings.

```{r first_bias}

#####
#Add in first bias
#####

#Define string with approach, used in tibble/chart heading
strApproach <- "Average+Movie"

#create dataframe, group by MovieID and then calculate mean rating with each movie. Estimate the least squares estimate for this value as b_i
movie_avgs <- train_set %>% 
  group_by(movieId) %>% 
  summarize(b_i = mean(rating - mu))

#plot data to visualise impact on ratings.
qplot(b_i, data = movie_avgs, bins = 10, color = I("black"))

#Merge and calculate the predicted rating for each movie into the test-data. 
predicted_ratings <- test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  mutate(pred = mu + b_i) %>%
  pull(pred)
  
#calculate the RSME from using the predicted ratings, and the actual ratings.
rmse = RMSE(predicted_ratings, test_set$rating)

# add a row to our results tibble with the calculated RSME for this method
rmse_results <- rmse_results %>% add_row(tibble_row(method = strApproach, RMSE = as.character(round(rmse,5))))

#print target RSME, and current approach RSME
kable(rmse_results[c(which(rmse_results$method=="Target"), which(rmse_results$method==strApproach)),], "simple")
```

### Second Bias (User ID)

Now we add in our second bias: the User ID. This will shift the estimated rating based on the average rating from each users. From the plot we can see how this shifts the ratings.

```{r second_bias}

#Define string with approach, used in tibble/chart heading
strApproach <- "Average+Movie+User"

#create dataframe, group by UserID and then calculate mean rating with each movie. Estimate the least squares estimate for this value as b_u
user_avgs <- train_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))

#plot data to visualise impact on ratings.
qplot(b_u, data = user_avgs, bins = 10, color = I("black"))

#Merge and calculate the predicted rating for each movie into the test-data. 
  predicted_ratings <- test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = mu + b_i + b_u) %>%
  pull(pred)
  
#calculate the RSME from using the predicted ratings, and the actual ratings.
rmse = RMSE(predicted_ratings, test_set$rating)

# add a row to our results tibble with the calculated RSME for this method
rmse_results <- rmse_results %>% add_row(tibble_row(method = strApproach, RMSE = as.character(round(rmse,5))))

#print target RSME, and current approach RSME
kable(rmse_results[c(which(rmse_results$method=="Target"), which(rmse_results$method==strApproach)),], "simple")
```


### Third Bias (Genre)

Now we add in our third bias: the Genre. This will shift the estimated rating based on the average rating for each genre. From the plot we can see this further shits the ratings.


```{r third_bias}

#####
#Add in third bias
#####

#Define string with approach, used in tibble/chart heading
strApproach <- "Average+Movie+User+Genre"

#create dataframe, group by Genre and then calculate mean rating with each movie. Estimate the least squares estimate for this value as b_g
genre_avgs <- train_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  group_by(genres) %>%
  summarize(b_g = mean(rating - mu - b_i - b_u))

#plot data to visualise impact on ratings.
qplot(b_g, data = genre_avgs, bins = 10, color = I("black"))

#Merge and calculate the predicted rating for each movie into the test-data. 
  predicted_ratings <- test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  left_join(genre_avgs, by='genres') %>%
  mutate(pred = mu + b_i + b_u + b_g) %>%
  pull(pred)
  
#calculate the RSME from using the predicted ratings, and the actual ratings.
rmse = RMSE(predicted_ratings, test_set$rating)

# add a row to our results tibble with the calculated RSME for this method
rmse_results <- rmse_results %>% add_row(tibble_row(method = strApproach, RMSE = as.character(round(rmse,5))))

#print target RSME, and RSME for current approach
kable(rmse_results[c(which(rmse_results$method=="Target"), which(rmse_results$method==strApproach)),], "simple")

```

### Fourth Bias (Release Year)

Now we add in our fourth bias: the Year Released. This will shift the estimated rating based on the year the movie was released. From the plot we can see this shifts the ratings.

```{r fourth_bias}

#####
#Add in fourth bias
#####

#Define string with approach, used in tibble/chart heading
strApproach <- "Average+Movie+User+Genre+Year Released"

#create dataframe, group by Released Year and then calculate mean rating with each movie. Estimate the least squares estimate for this value as b_y_rel.
year_rel_avgs <- train_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(genre_avgs, by='genres') %>%
  left_join(user_avgs, by='userId') %>%
  group_by(year_released) %>% 
  summarize(b_y_rel = mean(rating - mu - b_i - b_g - b_u))

#plot data to visualise impact on ratings.
qplot(b_y_rel, data = year_rel_avgs, bins = 10, color = I("black"))

#Merge and calculate the predicted rating for each movie into the test-data. 
  predicted_ratings <- test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  left_join(genre_avgs, by='genres') %>%
  left_join(year_rel_avgs, by='year_released') %>%
  mutate(pred = mu + b_i + b_u + b_g + b_y_rel) %>%
  pull(pred)

#calculate the RSME from using the predicted ratings, and the actual ratings.
rmse = RMSE(predicted_ratings, test_set$rating)

# add a row to our results tibble with the calculated RSME for this method
rmse_results <- rmse_results %>% add_row(tibble_row(method = strApproach, RMSE = as.character(round(rmse,5))))

#print target RSME, and RSME for current approach
kable(rmse_results[c(which(rmse_results$method=="Target"), which(rmse_results$method==strApproach)),],"simple")

```

### Fifth Bias (Year Rated)


Now we add in our fifth bias: the Year Rated This will shift the estimated rating based on the year the movie was rated. From the plot we can see this improves shifts the ratings.

```{r fifth_bias}

#####
#Add in fifth bias
#####

#Define string with approach, used in tibble/chart heading
strApproach <- "Average+Movie+User+Genre+Year Rel+Year Rated"

#create dataframe, group by Rated Year and then calculate mean rating with each movie. Estimate the least squares estimate for this value as b_y_rate.
year_rate_avgs <- train_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(genre_avgs, by='genres') %>%
  left_join(user_avgs, by='userId') %>%
  left_join(year_rel_avgs, by='year_released') %>%
  group_by(year_rate) %>% 
  summarize(b_y_rate = mean(rating - mu - b_i - b_g - b_u))
 
#plot data to visualise impact on ratings.
qplot(b_y_rate, data = year_rate_avgs, bins = 10, color = I("black"))

#Merge and calculate the predicted rating for each movie into the test-data. 
  predicted_ratings <- test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  left_join(genre_avgs, by='genres') %>%
  left_join(year_rel_avgs, by='year_released') %>%
  left_join(year_rate_avgs, by='year_rate') %>%
  mutate(pred = mu + b_i + b_u + b_g + b_y_rel + b_y_rate) %>%
  pull(pred)

  #calculate the RSME from using the predicted ratings, and the actual ratings.
rmse = RMSE(predicted_ratings, test_set$rating)

# add a row to our results tibble with the calculated RSME for this method
rmse_results <- rmse_results %>% add_row(tibble_row(method = strApproach, RMSE = as.character(round(rmse,5))))

#print target RSME, and RSME for current approach
kable(rmse_results[c(which(rmse_results$method=="Target"), which(rmse_results$method==strApproach)),], "simple")

```

### Summary of Model Biases

Summary of all biases shown below. Summary shows how each bias improves the RMSE, reaching a level below the target.  
```{r summary}

#####
#Show summary of RSME so far
#####
kable(rmse_results, "simple", digits = 6)
```


### Regularisation Optimisation

Regularisation can optimise the model by penalising large estimates that are formed using small sample sizes. The following explores how regularisation impacts the final RSME value. Regularisation is considered a tuning parameter, with small changes applied to this parameter, until the lowest RSME is identified. 
 

```{r regularisation}

#####
#Regularisation
#####

#define a variable to store optimum regularisation tuning paramater, and the associated RSME for each regularisation paramater.
opt_lambda <-
  data.frame(
    bias = c("b_i", "b_u", "b_g", "b_y_rel", "b_y_rate"),
    lambda = c(0, 0, 0, 0, 0),
    rsme = c(0, 0, 0, 0, 0)
  )

#function to run the model using different tuning parameter (l_b_i, l_b_u etc). blnOutOfBoundValues is a TRUE/FALSE parameter to determine if we also need to remove out of bound values (explain in code below)
optimise_rmse <- function(l_b_i, l_b_u, l_b_g, l_b_y_rel, l_b_y_rate, blnOutOfBoundValues) {
  
  #Calculate average rating in training data set
  mu <- mean(train_set$rating)
  
  #Add in movieID bias, with  regularisation parameter (n/l_b_i)
  b_i <- train_set %>%
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu) / (n() + l_b_i))
  
  #Add in userID bias, with  regularisation parameter (n/l_b_U)
  b_u <- train_set %>%
    left_join(b_i, by = "movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu) / (n() + l_b_u))
  
  #Add in Genre bias, with  regularisation parameter (n/l_b_g)
  b_g <- train_set %>%
    left_join(b_i, by = 'movieId') %>%
    left_join(b_u, by = 'userId') %>%
    group_by(genres) %>%
    summarize(b_g = sum(rating - mu - b_i - b_u) / (n() + l_b_g))

#Add in Year Released bias, with  regularisation parameter (n/l_b_rel). Note: This paramater has not been tuned as it was expected to provided minimal benifit, with a long high run time.
  b_y_rel <- train_set %>%
    left_join(b_i, by = 'movieId') %>%
    left_join(b_g, by = 'genres') %>%
    left_join(b_u, by = 'userId') %>%
    group_by(year_released) %>%
    summarize(b_y_rel = sum(rating - mu - b_i - b_g - b_u) / (n() + l_b_y_rel))
  
  #Add in Year Rate bias, with  regularisation parameter (n/l_b_rate). Note: This paramater has not been tuned as it was expected to provided minimal benifit, with a long high run time.
  b_y_rate <- train_set %>%
    left_join(b_i, by = 'movieId') %>%
    left_join(b_g, by = 'genres') %>%
    left_join(b_u, by = 'userId') %>%
    left_join(b_y_rel, by = 'year_released') %>%
    group_by(year_rate) %>%
    summarize(b_y_rate = sum(rating - mu - b_i - b_g - b_u) / (n() + l_b_y_rate))
  
  #Merge and calculate the predicted rating for each movie into the test-data. 
  predicted_ratings <- test_set %>%
    left_join(b_i, by = 'movieId') %>%
    left_join(b_u, by = 'userId') %>%
    left_join(b_g, by = 'genres') %>%
    left_join(b_y_rel, by = 'year_released') %>%
    left_join(b_y_rate, by = 'year_rate') %>%
    mutate(pred = mu + b_i + b_u + b_g + b_y_rel + b_y_rate) %>%
    pull(pred)
  
  #If required, all values less than 0 are converted to 0.5, and all values above 5 are converted to a value of 5. This aligns with initial data exploration.
  if (blnOutOfBoundValues == TRUE) {
    predicted_ratings <-
      replace(predicted_ratings, predicted_ratings <= 0, 0.5)
    predicted_ratings <-
      replace(predicted_ratings, predicted_ratings > 5, 5)
  }
 
  #Return the RSME from using the predicted ratings, and the actual ratings.
  return(RMSE(predicted_ratings, test_set$rating))
}

```

### Movie ID Regularisation
The chart shows how Movie ID regularisation improves the final RSME. The optimal tuning parameter is identified as the minimum value on the chart. 

```{r movie_id_reg}

#####
#Movie ID Regularisation
#####

#Define string with approach, used in tibble/chart heading
strApproach <- "All Biases + Movie ID Regularisation"

#Sequence of tuning parameters. Range of these values were initially a lot larger, however reduced to speed up running, and only required for demonstration purposes.
lambdas <- seq(4.6, 5.0, 0.1)

#Apply each tuning parameter (lambdas) into the model function defined above. 
rmses <- sapply(lambdas, function(l){
  optimise_rmse(l, 0, 0, 0, 0, FALSE)
})

#plot tuning parameter against calculate RSME to visualise improvement/change in RSME for each lambda
qplot(lambdas, rmses, main=strApproach)

#Store minimum RMSE and the optimal lambda for use in final model
opt_lambda[opt_lambda$bias == "b_i", 2] <- lambdas[which.min(rmses)]
opt_lambda[opt_lambda$bias == "b_i", 3] <- min(rmses)

# add a row to our results tibble with the calculated RSME for this method
rmse_results <- rmse_results %>% add_row(tibble_row(method = strApproach, RMSE = as.character(round(min(rmses),5))))

#print target RSME, and RSME for current approach
kable(rmse_results[c(which(rmse_results$method=="Target"), which(rmse_results$method==strApproach)),], "simple")
```

This tuning parameter used for the Movie ID regularisation is:
```{r min_rsme_movie}
#Print tuning parameter for movieID bias
kable(lambdas[which.min(rmses)], "simple", align = "l")
```


### User ID Regularisation
```{r user_id_reg}

#####
#User ID Regularisation
#####

#Define string with approach, used in tibble/chart heading
strApproach <- "All Biases + User ID Regularisation"

#Sequence of tuning parameters. Range of these values were initially a lot larger, however reduced to speed up running, and only required for demonstration purposes.
lambdas <- seq(4.7, 5.3, 0.1)

#Apply each tuning parameter (lambdas) into the model function defined above. 
rmses <- sapply(lambdas, function(l){
  optimise_rmse(0, l, 0, 0, 0, FALSE)
})

#plot tuning parameter against calculate RSME to visualise improvement/change in RSME for each lambda
qplot(lambdas, rmses, main=strApproach)

#Store minimum RMSE and the optimal lambda for use in final model
opt_lambda[opt_lambda$bias == "b_u", 2] <- lambdas[which.min(rmses)]
opt_lambda[opt_lambda$bias == "b_u", 3] <- min(rmses)

# add a row to our results tibble with the calculated RSME for this method
rmse_results <- rmse_results %>% add_row(tibble_row(method = strApproach, RMSE = as.character(round(min(rmses),5))))

#print target RSME, and RSME for current approach
kable(rmse_results[c(which(rmse_results$method=="Target"), which(rmse_results$method==strApproach)),], "simple")
```

This tuning parameter used for the User ID regularisation is:
```{r min_rsme_user}
#Print tuning paramater for userID bias
kable(lambdas[which.min(rmses)], "simple", align = "l")
```

### Genre Regularisation
```{r genre_id_reg}

#####
#Genre Regularisation
#####

#Define string with approach, used in tibble/chart heading
strApproach <- "All Biases + Genre Regularisation"

#Sequence of tuning parameters. Range of these values were initially a lot larger, however reduced to speed up running, and only required for demonstration purposes.
lambdas <- seq(0.4, 0.8, 0.1)

#Apply each tuning parameter (lambdas) into the model function defined above. 
rmses <- sapply(lambdas, function(l){
  optimise_rmse(0, 0, l, 0, 0, FALSE)
})

#plot tuning parameter against calculate RSME to visualise improvement/change in RSME for each lambda
qplot(lambdas, rmses, main=strApproach)

#Store minimum RMSE and the optimal lambda for use in final model
opt_lambda[opt_lambda$bias == "b_g", 2] <- lambdas[which.min(rmses)]
opt_lambda[opt_lambda$bias == "b_g", 3] <- min(rmses)

# add a row to our results tibble with the calculated RSME for this method
rmse_results <- rmse_results %>% add_row(tibble_row(method = strApproach, RMSE = as.character(round(min(rmses),5))))

#print target RSME, and RSME for current approach
kable(rmse_results[c(which(rmse_results$method=="Target"), which(rmse_results$method==strApproach)),], "simple")
```

This tuning parameter used for the Genre ID regularisation is:
```{r min_rsme_genre}
#Print tuning paramater for Genre bias
kable(lambdas[which.min(rmses)], "simple", align = "l")
```

### Combined User ID and Movie ID Regularisation
Now check with the values combined
```{r reg_combined}

#####
#All Biases+ All Regularisation
#####

#Define string with approach, used in tibble/chart heading
strApproach <- "All Biases + All Regularisation"

#Store optimum regularisation value calculated previously in variables
optimum_lambda_b_i <- opt_lambda[opt_lambda$bias == "b_i", 2]
optimum_lambda_b_u <- opt_lambda[opt_lambda$bias == "b_u", 2]
optimum_lambda_b_g <- opt_lambda[opt_lambda$bias == "b_g", 2]

#run model with these optimum values. Out of bound adjustment in model is set to FALSE
rmses <- optimise_rmse(optimum_lambda_b_i, optimum_lambda_b_u, optimum_lambda_b_g, 0, 0, FALSE)

# add a row to our results tibble with the calculated RSME for this method
rmse_results <- rmse_results %>% add_row(tibble_row(method = strApproach, RMSE = as.character(round(rmses,5))))

#print target RSME, and RSME for current approach
kable(rmse_results[c(which(rmse_results$method=="Target"), which(rmse_results$method==strApproach)),], "simple")

```

Based on the initial data investigation we determined that no values were below 0.5, and no values were above 5. Based on this information we should also adjust the final dataset to ensure no values are above or below this values. Where a result is less than or equal to 0, then it is changed to 0.5. Similarly, results above 5 are changed to 5. 

```{r out_of_bounds}

#####
#All Biases + All Regularisation + Out of Range Adjustment
#####

#Define string with approach, used in tibble/chart heading
strApproach <- "All Biases + All Regularisation + Out of Range Adjustment"

#Store optimum regularisation value calculated previously in variables
optimum_lambda_b_i <- opt_lambda[opt_lambda$bias == "b_i", 2]
optimum_lambda_b_u <- opt_lambda[opt_lambda$bias == "b_u", 2]
optimum_lambda_b_g <- opt_lambda[opt_lambda$bias == "b_g", 2]

#run model with these optimum values. Out of bound adjustment in model is now set to TRUE
rmses <- optimise_rmse(optimum_lambda_b_i, optimum_lambda_b_u, optimum_lambda_b_g, 0 , 0, TRUE)

# add a row to our results tibble with the calculated RSME for this method
rmse_results <- rmse_results %>% add_row(tibble_row(method = strApproach, RMSE = as.character(round(rmses,5))))

#print target RSME, and RSME for current approach
kable(rmse_results[c(which(rmse_results$method=="Target"), which(rmse_results$method==strApproach)),], "simple")
```

### Summary of all models

The below table summarises the RSME for all models that have been evaluated.
```{r Summary_all}
#####
#Show summary of RSME for all data
#####

#print all RSME results as a final summary
kable(rmse_results, "simple", digits = 6)
```


### Final Evaluation Using Validation Dataset
Based on the optimisations above, we now use a model that applies all biases, optimises using the regularisation of Movie ID and User ID, and remove the out of range data. As the final step in the project this model is applied to the validation data set to evaluate the final RSME. 

```{r final_data_eval}

#####
#Final model run against validation data set 
#####

#Define string with approach, used in tibble/chart heading
strApproach <- "Validation Data"

#Store optimum regularisation value calculated previously in variables
optimum_lambda_b_i <- opt_lambda[opt_lambda$bias == "b_i", 2]
optimum_lambda_b_u <- opt_lambda[opt_lambda$bias == "b_u", 2]
optimum_lambda_b_g <- opt_lambda[opt_lambda$bias == "b_g", 2]

  #Calculate average rating in training data set
  mu <- mean(train_set$rating)
  
  #Add in movieID bias, with  regularisation parameter (n/l_b_i)
  b_i <- train_set %>%
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu) / (n() + optimum_lambda_b_i))
  
  #Add in UserID bias, with  regularisation parameter (n/l_b_u)
  b_u <- train_set %>%
    left_join(b_i, by = "movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu) / (n() + optimum_lambda_b_u))
  
    #Add in Genre bias, with  regularisation parameter (n/l_b_g)
  b_g <- train_set %>%
    left_join(b_i, by = 'movieId') %>%
    left_join(b_u, by = 'userId') %>%
    group_by(genres) %>%
    summarize(b_g = sum(rating - mu - b_i - b_u) / (n() + optimum_lambda_b_g))
  
    #Add in Year Release bias (no regularisation)
  b_y_rel <- train_set %>%
    left_join(b_i, by = 'movieId') %>%
    left_join(b_g, by = 'genres') %>%
    left_join(b_u, by = 'userId') %>%
    group_by(year_released) %>%
    summarize(b_y_rel = sum(rating - mu - b_i - b_g - b_u) / (n()))
  
    #Add in Year Rated bias (no regularisation)
  b_y_rate <- train_set %>%
    left_join(b_i, by = 'movieId') %>%
    left_join(b_g, by = 'genres') %>%
    left_join(b_u, by = 'userId') %>%
    left_join(b_y_rel, by = 'year_released') %>%
    group_by(year_rate) %>%
    summarize(b_y_rate = sum(rating - mu - b_i - b_g - b_u) / (n()))
  
  #Merge and calculate the predicted rating for each movie into the validation-data. 
  predicted_ratings <- validation_set %>%
    left_join(b_i, by = 'movieId') %>%
    left_join(b_u, by = 'userId') %>%
    left_join(b_g, by = 'genres') %>%
    left_join(b_y_rel, by = 'year_released') %>%
    left_join(b_y_rate, by = 'year_rate') %>%
    mutate(pred = mu + b_i + b_u + b_g + b_y_rel + b_y_rate) %>%
    pull(pred)
  
  #All values less than 0 are converted to 0.5, and all values above 5 are converted to a value of 5. This aligns with range of ratings in initial data exploration.
    predicted_ratings <-
      replace(predicted_ratings, predicted_ratings <= 0, 0.5)
    predicted_ratings <-
      replace(predicted_ratings, predicted_ratings > 5, 5)
  
  #calculate the final RMSE using the predicted values against the know validation data set
    final_RMSE <- RMSE(predicted_ratings, validation_set$rating)
    
    #add a row to our results tibble with the calculated RSME for this method
    rmse_results <- rmse_results %>% add_row(tibble_row(method = strApproach, RMSE = as.character(round(final_RMSE,5))))

#print target RSME, and RSME for current approach
kable(rmse_results[c(which(rmse_results$method=="Target"), which(rmse_results$method==strApproach)),], "simple")

  
```

```{r end_time}
#save the time this code finished running. This is to calculate total run time for this model.
end_time <- Sys.time()
```

## Conclusion
<!-- section that gives a brief summary of the report, its limitations and future work -->
The final RSME was:
```{r Final_RMSE_Conclusion}
#####
#Conclusions
#####

#String defining the approach
strApproach <- "Validation Data"

#print target RSME, and RSME for current approach
kable(rmse_results[c(which(rmse_results$method=="Target"), which(rmse_results$method==strApproach)),], "simple")
```

This is below the target of 0.86490, and so our model is working as per the initial scope of the project. This model uses biases, regularisation and a range cut-off to optimise the model. 

Future work could explore the use of a matrix factorisation technique, or other machine learning algorithms such as Collaborative Filtering or Neural Networks. 

Run time was a limitation with this model, with compile time exceeding 30 minutes even using caching techniques, making it difficult to quickly iterate and trial different models. Updated hardware or a reduced data-set is necessary to further investigate different modeling techniques. 

```{r run_time}
#####
#Run Time
#####

#calculate and display total running time (in minutes) using start time and end time. 
kable(round(as.double(difftime(end_time, start_time, units = "mins")),1), format="simple", row.names=FALSE, caption="Code Running Time (min)", align = "l")
```
